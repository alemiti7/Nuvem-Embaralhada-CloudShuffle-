import pandas as pd
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
import logging 
from collections import defaultdict
import random
from datetime import datetime

# ======================================================================
# DESCRI√á√ÉO GERAL DO SCRIPT - VERS√ÉO COM RANDOMIZA√á√ÉO E TIMESTAMP
# ======================================================================
# Este script Python realiza o processamento de um arquivo CSV para gerar
# uma Nuvem de Palavras (WordCloud) com RANDOMIZA√á√ÉO e TIMESTAMP autom√°tico.
#
# FUNCIONALIDADES PRINCIPAIS:
# 1. Processamento em chunks para datasets grandes (>100k registros)
# 2. Randomiza√ß√£o: Embaralha os termos a cada execu√ß√£o (visualiza√ß√£o diferente)
# 3. Timestamp: Gera arquivo √∫nico com data/hora (nunca sobrescreve)
# 4. Sistema de logging completo para auditoria e debug
#
# BIBLIOTECAS UTILIZADAS:
# - pandas: Leitura e manipula√ß√£o de dados CSV com suporte a chunks
# - sklearn: Vetoriza√ß√£o e contagem de frequ√™ncia de termos
# - wordcloud: Gera√ß√£o da visualiza√ß√£o em nuvem de palavras
# - logging: Sistema de rastreamento e registro de eventos
# - collections.defaultdict: Acumula√ß√£o eficiente de frequ√™ncias
# - random: Randomiza√ß√£o dos termos para cada execu√ß√£o
# - datetime: Gera√ß√£o de timestamp para nomes √∫nicos de arquivo
#
# OTIMIZA√á√ïES IMPLEMENTADAS:
# - Processamento em chunks para datasets massivos (evita overflow de mem√≥ria)
# - Uso de matriz esparsa para redu√ß√£o de mem√≥ria (60-80%)
# - Acumula√ß√£o incremental de frequ√™ncias entre chunks
# - Otimiza√ß√£o de tipos de dados (dtype) para economia de RAM
# - Resolu√ß√£o de imagem ajustada para melhor performance
#
# CAPACIDADE:
# - Datasets pequenos (<100k): Processamento padr√£o r√°pido
# - Datasets m√©dios (100k-1M): Processamento em chunks de 50k
# - Datasets grandes (>1M): Processamento em chunks de 20k-50k
#
# FORMATO DE SA√çDA:
# - Arquivo: myplot_YYYYMMDD_HHMMSS.png
# - Exemplo: myplot_20251109_143025.png
# ======================================================================

# ----------------------------------------------------------------------
# ‚öôÔ∏è BLOCO 1: CONFIGURA√á√ÉO DO SISTEMA DE LOGGING
# ----------------------------------------------------------------------
LOG_FILE = 'script_wordcloud.log'
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    filemode='a'
)

logger = logging.getLogger(__name__)
logger.info("--- In√≠cio da execu√ß√£o do script de Nuvem de Palavras (COM RANDOMIZA√á√ÉO) ---")
# ----------------------------------------------------------------------


# ----------------------------------------------------------------------
# üìÅ BLOCO 2: DEFINI√á√ÉO DE CONFIGURA√á√ïES INICIAIS
# ----------------------------------------------------------------------
file_path = 'heroes_information.csv'
column_name = 'name'
CHUNK_SIZE = 50000

# üé≤ NOVA CONFIGURA√á√ÉO: Seed aleat√≥ria para randomiza√ß√£o
random.seed()  # Usa timestamp como seed (diferente a cada execu√ß√£o)

logger.info(f"Configura√ß√£o: Arquivo='{file_path}', Coluna='{column_name}', Chunk={CHUNK_SIZE}")
logger.info("üé≤ Modo de randomiza√ß√£o ativado")
# ----------------------------------------------------------------------


# ----------------------------------------------------------------------
# üîß BLOCO 3: FUN√á√ÉO TOKENIZADORA PERSONALIZADA
# ----------------------------------------------------------------------
def comma_tokenizer(text):
    """
    Tokeniza texto separado por v√≠rgulas.
    
    Args:
        text (str): Texto a ser tokenizado
        
    Returns:
        list: Lista de tokens limpos e n√£o vazios
    """
    if pd.isna(text):
        return []
    
    tokens = [t.strip() for t in str(text).split(',') if t.strip()]
    return tokens
# ----------------------------------------------------------------------


# ----------------------------------------------------------------------
# üìä BLOCO 4: INICIALIZA√á√ÉO DE ESTRUTURAS DE DADOS
# ----------------------------------------------------------------------
global_vocab = set()
freq_accumulator = defaultdict(int)
total_rows_processed = 0
total_rows_valid = 0
chunk_count = 0

logger.info("Estruturas de dados inicializadas para processamento em chunks.")
# ----------------------------------------------------------------------


# ----------------------------------------------------------------------
# üìñ BLOCO 5: LEITURA E PROCESSAMENTO EM CHUNKS
# ----------------------------------------------------------------------
try:
    logger.info("Iniciando leitura do arquivo em chunks...")
    
    for chunk_num, chunk in enumerate(pd.read_csv(
        file_path,
        encoding='utf-8',
        usecols=[column_name],
        dtype={column_name: 'string'},
        chunksize=CHUNK_SIZE
    ), start=1):
        
        logger.info(f"Processando chunk {chunk_num} ({len(chunk)} linhas)...")
        
        if column_name not in chunk.columns:
            logger.error(f"Coluna '{column_name}' n√£o encontrada no chunk {chunk_num}.")
            print(f"‚ùå Erro: Coluna '{column_name}' n√£o encontrada.")
            exit()
        
        texts_chunk = chunk[column_name].dropna()
        valid_in_chunk = len(texts_chunk)
        
        total_rows_processed += len(chunk)
        total_rows_valid += valid_in_chunk
        
        logger.info(f"  ‚Üí Chunk {chunk_num}: {valid_in_chunk} linhas v√°lidas de {len(chunk)}")
        
        if valid_in_chunk == 0:
            logger.warning(f"  ‚ö†Ô∏è Chunk {chunk_num} vazio ap√≥s remo√ß√£o de NaN. Pulando...")
            continue
        
        vectorizer_chunk = CountVectorizer(tokenizer=comma_tokenizer)
        X_chunk = vectorizer_chunk.fit_transform(texts_chunk)
        
        freq_chunk = X_chunk.sum(axis=0).A1
        terms_chunk = vectorizer_chunk.get_feature_names_out()
        
        logger.info(f"  ‚Üí Chunk {chunk_num}: {len(terms_chunk)} termos √∫nicos encontrados")
        
        for term, freq in zip(terms_chunk, freq_chunk):
            global_vocab.add(term)
            freq_accumulator[term] += freq
        
        chunk_count += 1
        logger.info(f"  ‚úÖ Chunk {chunk_num} processado. Total de termos √∫nicos: {len(global_vocab)}")
        
        del chunk, texts_chunk, X_chunk, vectorizer_chunk
    
    logger.info(f"‚úÖ Todos os chunks processados com sucesso!")
    logger.info(f"üìä ESTAT√çSTICAS FINAIS:")
    logger.info(f"  - Total de chunks processados: {chunk_count}")
    logger.info(f"  - Total de linhas no arquivo: {total_rows_processed}")
    logger.info(f"  - Total de linhas v√°lidas (sem NaN): {total_rows_valid}")
    logger.info(f"  - Total de termos √∫nicos: {len(global_vocab)}")
    logger.info(f"  - Taxa de linhas v√°lidas: {(total_rows_valid/total_rows_processed*100):.2f}%")

except FileNotFoundError:
    logger.error(f"‚ùå Erro: O arquivo '{file_path}' n√£o foi encontrado.")
    print(f"‚ùå Erro: O arquivo '{file_path}' n√£o foi encontrado.")
    exit()

except pd.errors.EmptyDataError:
    logger.error(f"‚ùå Erro: O arquivo '{file_path}' est√° vazio ou corrompido.")
    print(f"‚ùå Erro: Arquivo vazio ou corrompido.")
    exit()
    
except Exception as e:
    logger.critical(f"‚ùå Erro inesperado durante o processamento: {e}")
    print(f"‚ùå Erro inesperado: {e}")
    exit()
# ----------------------------------------------------------------------


# ----------------------------------------------------------------------
# ‚úÖ BLOCO 6: VALIDA√á√ÉO DE DADOS PROCESSADOS
# ----------------------------------------------------------------------
if len(freq_accumulator) == 0:
    logger.error("‚ùå Nenhum termo v√°lido encontrado ap√≥s processar todos os chunks.")
    print("‚ùå Erro: Nenhum dado v√°lido para gerar nuvem de palavras.")
    exit()

logger.info(f"Valida√ß√£o conclu√≠da. Prosseguindo com {len(freq_accumulator)} termos.")
# ----------------------------------------------------------------------


# ----------------------------------------------------------------------
# üé≤ BLOCO 6.5: RANDOMIZA√á√ÉO DAS FREQU√äNCIAS
# ----------------------------------------------------------------------
# NOVA FUNCIONALIDADE: Embaralha as frequ√™ncias entre os termos
# Mant√©m a distribui√ß√£o de tamanhos mas randomiza quais palavras
# aparecem em cada tamanho
# ----------------------------------------------------------------------
logger.info("üé≤ Aplicando randomiza√ß√£o nas frequ√™ncias...")

# Extrai listas separadas de termos e frequ√™ncias
terms_list = list(freq_accumulator.keys())
freq_list = list(freq_accumulator.values())

# Embaralha a lista de termos (mant√©m frequ√™ncias na ordem original)
random.shuffle(terms_list)

# Recria o dicion√°rio com termos embaralhados mas frequ√™ncias originais
randomized_freq = dict(zip(terms_list, freq_list))

logger.info(f"‚úÖ Frequ√™ncias randomizadas. {len(randomized_freq)} termos redistribu√≠dos.")
logger.info(f"üéØ Preview - Termos mais frequentes ap√≥s randomiza√ß√£o: {list(randomized_freq.keys())[:10]}")
# ----------------------------------------------------------------------


# ----------------------------------------------------------------------
# üé® BLOCO 7: GERA√á√ÉO DA NUVEM DE PALAVRAS
# ----------------------------------------------------------------------
logger.info("Gerando nuvem de palavras a partir das frequ√™ncias randomizadas...")

wordcloud = WordCloud(
    width=1024,
    height=768,
    background_color='white',
    max_words=500,
    min_word_length=2,
    relative_scaling=0.5,
).generate_from_frequencies(randomized_freq)  # üé≤ Usa frequ√™ncias randomizadas

logger.info("Objeto WordCloud gerado com sucesso a partir das frequ√™ncias randomizadas.")
# ----------------------------------------------------------------------


# ----------------------------------------------------------------------
# üíæ BLOCO 8: SALVAMENTO DA IMAGEM COM TIMESTAMP
# ----------------------------------------------------------------------
# üïê NOVO: Gera nome de arquivo com timestamp √∫nico
# Formato: myplot_YYYYMMDD_HHMMSS.png
# Exemplo: myplot_20251109_143025.png
# ----------------------------------------------------------------------
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_file = f'myplot_{timestamp}.png'

logger.info(f"üìÅ Nome do arquivo gerado: {output_file}")

try:
    wordcloud.to_file(output_file)
    logger.info(f"Nuvem de palavras salva com sucesso como '{output_file}'.")
    print(f"‚úÖ Nuvem de palavras gerada e salva como '{output_file}'.")
    print(f"üïê Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üé≤ Modo randomizado: Cada execu√ß√£o gera uma visualiza√ß√£o diferente!")
    print(f"üìä Total de termos processados: {len(global_vocab):,}")
    print(f"üìä Total de linhas processadas: {total_rows_processed:,}")
    print(f"üìä Chunks processados: {chunk_count}")
    
except Exception as e:
    logger.error(f"‚ùå Erro ao salvar a imagem da nuvem de palavras: {e}")
    print(f"‚ùå Erro ao salvar a imagem: {e}")
# ----------------------------------------------------------------------


# ----------------------------------------------------------------------
# üèÅ BLOCO 9: FINALIZA√á√ÉO DO SCRIPT
# ----------------------------------------------------------------------
logger.info("--- Fim da execu√ß√£o do script (COM RANDOMIZA√á√ÉO) ---")
# ----------------------------------------------------------------------


# ======================================================================
# üé≤ DOCUMENTA√á√ÉO DA RANDOMIZA√á√ÉO E TIMESTAMP
# ======================================================================
#
# ESTRAT√âGIA IMPLEMENTADA:
# - Coleta todas as frequ√™ncias originais dos termos
# - Embaralha aleatoriamente a lista de termos
# - Reassocia os termos embaralhados com as frequ√™ncias originais
# - Resultado: Mesma distribui√ß√£o de tamanhos, mas palavras diferentes
#
# üïê TIMESTAMP NOS ARQUIVOS:
# Formato: myplot_YYYYMMDD_HHMMSS.png
# Benef√≠cios:
# - Nunca sobrescreve arquivos anteriores
# - F√°cil identifica√ß√£o da ordem cronol√≥gica
# - Rastreamento de m√∫ltiplas execu√ß√µes
# - Compara√ß√£o visual entre diferentes gera√ß√µes
#
# Exemplos de nomes gerados:
# - myplot_20251109_143025.png  (09/11/2025 √†s 14:30:25)
# - myplot_20251109_150133.png  (09/11/2025 √†s 15:01:33)
# - myplot_20251110_091545.png  (10/11/2025 √†s 09:15:45)
#
# EXEMPLO DE RANDOMIZA√á√ÉO:
# Original:  {"batman": 100, "superman": 80, "spider-man": 60}
# Shuffle:   {"spider-man": 100, "batman": 80, "superman": 60}
# (Cada execu√ß√£o produz uma redistribui√ß√£o diferente)
#
# ALTERNATIVAS POSS√çVEIS:
# 1. random.seed(42) - Para resultados reproduz√≠veis com seed fixa
# 2. Randomiza√ß√£o parcial - Embaralhar apenas top N termos
# 3. Randomiza√ß√£o de cores - Al√©m de posi√ß√£o/tamanho
# 4. Sampling - Mostrar subset aleat√≥rio diferente a cada execu√ß√£o
#
# FORMATOS DE TIMESTAMP ALTERNATIVOS:
# - ISO 8601: datetime.now().isoformat() ‚Üí "2025-11-09T14:30:25"
# - Unix: int(time.time()) ‚Üí "1731167425"
# - Leg√≠vel: strftime("%d-%b-%Y_%Hh%Mm%Ss") ‚Üí "09-Nov-2025_14h30m25s"
#
# Para SEED FIXA (resultados reproduz√≠veis):
# Substitua random.seed() por random.seed(42) no Bloco 2
#
# ======================================================================